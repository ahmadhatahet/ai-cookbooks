# Utilizing LoRA (Low-Rank Adaptation) for fine-tuning an LLM

LoRA is a very intuitive technique to customize an LLM for specific task without the need to full fine-tune a language model. More about it [here](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)

<img src=https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png width=600px alt="Visualization of how LoRA modify LLM parameters"/>